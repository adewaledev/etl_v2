# Dockerized Python ETL Pipeline

This repository contains a complete, automated, and Dockerized **ETL (Extract, Transform, Load) pipeline**.  
The pipeline reads data from a CSV file, transforms it, and loads it into a PostgreSQL database running in a separate Docker container.  

The project demonstrates key concepts of **Docker**, including containerization, networking, and orchestration â€” all managed by a single bash script.  
This version enhances security by **storing sensitive database credentials in a `.env` file** instead of hardcoding them.  

------------------------------------------------------------------

## Table of Contents

- [Prerequisites] (#-prerequisites)  
- [Project Structure] (#-project-structure)  
- [Environment Variables] (#-environment-variables)  
- [Getting Started] (#-getting-started)  
- [Pipeline Components] (#-pipeline-components)  

------------------------------------------------------------------

ðŸ”§ Prerequisites
To run this pipeline, you need:  

- **Docker**: Ensure Docker Desktop or Docker Engine is installed and running.  
- **Bash**: A bash-compatible shell (standard on Linux/macOS, available via Git Bash on Windows).  

------------------------------------------------------------------

## Project Structure

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                 # Environment variables (DB credentials)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ people-100.csv   # Input dataset
â”œâ”€â”€ docker-db/
â”‚   â””â”€â”€ Dockerfile       # PostgreSQL image setup
â”œâ”€â”€ docker-etl/
â”‚   â”œâ”€â”€ Dockerfile       # ETL image setup
â”‚   â”œâ”€â”€ etl_pipeline.py  # ETL process script
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â””â”€â”€ run_pipeline.sh      # Orchestration script
```

------------------------------------------------------------------

## Environment Variables

All sensitive configuration is stored in a `.env` file at the root of the project.  
Example `.env` file:  

```sh
DB_HOST=db
DB_NAME=etldb
DB_USER=user
DB_PASSWORD=pass
DB_PORT=5432
```

## Getting Started

1. **Clone the repository**  

    ```git
      git clone https://github.com/adewaledev/etl_v2
      cd etl_v2
    ```

2. **Set up environment variables**  
   Copy `.env.example` to `.env` and adjust values if needed:  
   cp .env.example .env

3. **Add the data file**  
   Place the provided `people-100.csv` inside the `data/` directory.  

4. **Run the automation script**  
   chmod +x run_pipeline.sh
   ./run_pipeline.sh

The script will:  
âœ… Build the Docker images  
âœ… Create a Docker network  
âœ… Start the PostgreSQL container (using credentials from `.env`)  
âœ… Run the ETL container (reads same `.env`)  
âœ… Load transformed data into the database  
âœ… Show confirmation of success  

------------------------------------------------------------------

## Pipeline Components

**.env**
Holds all database credentials and connection details.  

## **run_pipeline.sh**

- Orchestrates the pipeline.  
- Loads environment variables from `.env`.  
- Passes them securely to containers.  

## **docker-etl/etl_pipeline.py**

- Core ETL logic.  
- Reads credentials from environment variables.  
- Extracts â†’ Transforms â†’ Loads into Postgres.  

## **Dockerfiles**

- **docker-db/Dockerfile** â†’ Builds PostgreSQL container (generic, no credentials).  
- **docker-etl/Dockerfile** â†’ Builds ETL container with Python dependencies.  

------------------------------------------------------------------

## With this setup, the pipeline is  

- **Secure** â†’ credentials externalized in `.env`.  
- **Reproducible** â†’ same results every run.  
- **Isolated** â†’ runs fully inside Docker containers.
